{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced ML HW#3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftx3zZt8OHvd",
        "colab_type": "text"
      },
      "source": [
        "## Instructions:\n",
        "\n",
        "(Use the BBC News Category Classification Notebook in the Week 12 folder to import data for this assignment)\n",
        "\n",
        "Your final report should be written up in a Jupyter notebook.  It should be posted to a public Github repo as an ipynb AND it should be saved as an html file and submitted to this assignment via courseworks.  Please include the link to your Github repo in this html file.\n",
        "\n",
        "Your report should include the following information:\n",
        "\n",
        "(Note: Be sure to split your data into training and test sets after importing the csv file with pandas.  You can use sklearn's train_test_split() function to split your data. )\n",
        "\n",
        "1)  Visualize the categories of your target variable and describe the dataset generally (the data includes news articles from the BBC news.)  A simple description is fine.\n",
        "\n",
        "2) Preprocess your data such that each document in the data is represented as a sequence of equal length.\n",
        "\n",
        "3)  Use the data to fit separate models to each of the following architectures:\n",
        "\n",
        "A. A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)\n",
        "\n",
        "B. A model using an Embedding layer with Conv1d Layers\n",
        "\n",
        "C. A model using an Embedding layer with one sequential layer (LSTM or GRU)\n",
        "\n",
        "D. A model using an Embedding layer with stacked sequential layers (LSTM or GRU)\n",
        "\n",
        "E. A model using an Embedding layer with bidirectional sequential layers\n",
        "\n",
        "F. Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!).\n",
        "\n",
        "4) Discuss 1) which model(s) performed best and speculate about 2) how you might try to further improve the predictive power of your model (e.g. Glove embeddings? More layers? Combining Conv1D with LSTM layers? More LSTM hidden nodes?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v-vY12U8CWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoqSwgStNzT4",
        "colab_type": "code",
        "outputId": "a0ce7164-7db8-4a0b-b77d-9c1637f265f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df = pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDt2XqXe_Ufo",
        "colab_type": "text"
      },
      "source": [
        "## 1)  Visualize the categories of your target variable and describe the dataset generally (the data includes news articles from the BBC news.)  A simple description is fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC9tB-z20gWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2737f351-6d9f-4a47-878d-b50a7219aaef"
      },
      "source": [
        "min_length, max_length = 10000, 0\n",
        "\n",
        "length_total = 0\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "\n",
        "  length = len(df['text'].iloc[i].split())\n",
        "\n",
        "  length_total += length\n",
        "\n",
        "  if length < min_length:\n",
        "    min_length = length\n",
        "  \n",
        "  if length > max_length:\n",
        "    max_length = length\n",
        "\n",
        "print(f'The shortest text length is {min_length}, and the longest text length is {max_length}. \\nThe average text length is {round(length_total/df.shape[0], 1)}')\n",
        "\n",
        "print(f'The target classes are: {df[\"category\"].unique()}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length is 90, and the longest text length is 4492. \n",
            "The average text length is 390.3\n",
            "The target classes are: ['tech' 'business' 'sport' 'entertainment' 'politics']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueSfUoe_A9Qj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "c1cd8ef9-95ca-491d-aefa-34c8788626b8"
      },
      "source": [
        "df.groupby('category').count().plot.bar()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTQ8XD2C3HD",
        "colab_type": "text"
      },
      "source": [
        "There are five target categories: tech, business, sport, entertainment, and politics. The goal is to use the text of each to predict which category is being discussed. The average length in words of the text variable is 390.3; the minimum length is 90 words; the maximum length is 4,492. Each category is pretty evenly represented, with entertainment (the least common) having nearly 400 instances in the dataset while business (the most common) having just over 500 instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr6m4z0jDlOL",
        "colab_type": "text"
      },
      "source": [
        "## 2) Preprocess your data such that each document in the data is represented as a sequence of equal length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L2u_pZQ9PTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "texts = df['text']\n",
        "\n",
        "categories = df['category']\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "y = ohe.fit_transform(np.asarray(categories).reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MSRPWltye84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = 100\n",
        "\n",
        "max_words = 10000\n",
        "\n",
        "train_len = 1500\n",
        "\n",
        "val_len = 500\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "tok_text = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "data = pad_sequences(tok_text, maxlen=max_len)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "data = data[indices]\n",
        "labels = y[indices]\n",
        "\n",
        "X_train = data[:train_len]\n",
        "y_train = labels[:train_len]\n",
        "\n",
        "X_val = data[train_len: train_len + val_len]\n",
        "y_val = labels[train_len: train_len + val_len]\n",
        "\n",
        "X_test = data[train_len + val_len:]\n",
        "y_test = labels[train_len + val_len:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k-MixnwKogL",
        "colab_type": "text"
      },
      "source": [
        "## 3. Can you build a model... \n",
        "\n",
        "### a)... with an embedding layer and dense layers (but w/ no layers meant for sequential data)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss-XEZpJKb38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "14ea577a-0e9b-474f-a9ac-8ead7daf6092"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 0s 120us/step - loss: 1.6538 - accuracy: 0.2573 - val_loss: 1.5650 - val_accuracy: 0.2320\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 0s 42us/step - loss: 1.2758 - accuracy: 0.4400 - val_loss: 1.4306 - val_accuracy: 0.4320\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 0s 39us/step - loss: 0.8645 - accuracy: 0.9973 - val_loss: 1.2504 - val_accuracy: 0.6220\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 0s 40us/step - loss: 0.4864 - accuracy: 1.0000 - val_loss: 1.0209 - val_accuracy: 0.6960\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 0s 45us/step - loss: 0.2157 - accuracy: 1.0000 - val_loss: 0.8458 - val_accuracy: 0.7520\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 0s 40us/step - loss: 0.0913 - accuracy: 1.0000 - val_loss: 0.7321 - val_accuracy: 0.7960\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 0s 41us/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 0.8120\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 0s 40us/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 0.8120\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 0s 39us/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.5868 - val_accuracy: 0.8140\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 0s 39us/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.5722 - val_accuracy: 0.8120\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 0s 41us/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.5628 - val_accuracy: 0.8080\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 0s 38us/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.5558 - val_accuracy: 0.8100\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 0s 43us/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.5505 - val_accuracy: 0.8140\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 0s 41us/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.5459 - val_accuracy: 0.8140\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 0s 38us/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5416 - val_accuracy: 0.8160\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 0s 40us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5377 - val_accuracy: 0.8180\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 0s 38us/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5341 - val_accuracy: 0.8180\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 0s 39us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.5308 - val_accuracy: 0.8160\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 0s 38us/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5277 - val_accuracy: 0.8160\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 0s 39us/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5249 - val_accuracy: 0.8180\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f207ecb2be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CpKd0hsMfq0",
        "colab_type": "text"
      },
      "source": [
        "### b) ... a model using Conv1d Layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JitpWwsxLrLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "fdb2b930-185c-43d0-e655-e4191b6ac404"
      },
      "source": [
        "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "model.add(Conv1D(64, kernel_size=2, strides=1))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(256, kernel_size=4, strides=2))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 0s 187us/step - loss: 1.6021 - accuracy: 0.2420 - val_loss: 1.5859 - val_accuracy: 0.3600\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 0s 43us/step - loss: 1.4910 - accuracy: 0.4653 - val_loss: 1.4927 - val_accuracy: 0.4200\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 0s 43us/step - loss: 1.1475 - accuracy: 0.6367 - val_loss: 1.0656 - val_accuracy: 0.6300\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 0s 43us/step - loss: 0.5242 - accuracy: 0.9293 - val_loss: 0.6411 - val_accuracy: 0.7980\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 0s 45us/step - loss: 0.1345 - accuracy: 0.9847 - val_loss: 0.4756 - val_accuracy: 0.8380\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 0s 46us/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 0.8600\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 0s 43us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4608 - val_accuracy: 0.8660\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 0s 45us/step - loss: 5.0534e-04 - accuracy: 1.0000 - val_loss: 0.4999 - val_accuracy: 0.8700\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 0s 49us/step - loss: 2.0845e-04 - accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.8780\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 0s 43us/step - loss: 8.7674e-05 - accuracy: 1.0000 - val_loss: 0.5256 - val_accuracy: 0.8740\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 0s 43us/step - loss: 5.0806e-05 - accuracy: 1.0000 - val_loss: 0.5297 - val_accuracy: 0.8740\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 0s 46us/step - loss: 3.7479e-05 - accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.8740\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 0s 47us/step - loss: 3.0873e-05 - accuracy: 1.0000 - val_loss: 0.5326 - val_accuracy: 0.8740\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 0s 44us/step - loss: 2.7494e-05 - accuracy: 1.0000 - val_loss: 0.5326 - val_accuracy: 0.8740\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 0s 44us/step - loss: 2.4997e-05 - accuracy: 1.0000 - val_loss: 0.5327 - val_accuracy: 0.8740\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 0s 43us/step - loss: 2.3121e-05 - accuracy: 1.0000 - val_loss: 0.5327 - val_accuracy: 0.8740\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 0s 44us/step - loss: 2.1943e-05 - accuracy: 1.0000 - val_loss: 0.5328 - val_accuracy: 0.8740\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 0s 42us/step - loss: 2.0842e-05 - accuracy: 1.0000 - val_loss: 0.5330 - val_accuracy: 0.8740\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 0s 42us/step - loss: 2.0047e-05 - accuracy: 1.0000 - val_loss: 0.5332 - val_accuracy: 0.8740\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 0s 41us/step - loss: 1.9273e-05 - accuracy: 1.0000 - val_loss: 0.5332 - val_accuracy: 0.8720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1c3c2ebf60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffyfli0FOQjA",
        "colab_type": "text"
      },
      "source": [
        "### c) ... with one sequential layer (LSTM or GRU)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_nLj9h0OB4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "e1376af7-7ed5-4cf4-e774-a0fef724999a"
      },
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "model.add(GRU(256))\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 1.8526 - accuracy: 0.1687 - val_loss: 1.6683 - val_accuracy: 0.1780\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.6274 - accuracy: 0.2353 - val_loss: 1.6273 - val_accuracy: 0.2160\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.5716 - accuracy: 0.2400 - val_loss: 1.5844 - val_accuracy: 0.2240\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.5160 - accuracy: 0.2900 - val_loss: 1.5361 - val_accuracy: 0.3480\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.3760 - accuracy: 0.5207 - val_loss: 1.3618 - val_accuracy: 0.4560\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.0515 - accuracy: 0.5720 - val_loss: 1.1725 - val_accuracy: 0.5860\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.7892 - accuracy: 0.8647 - val_loss: 0.9826 - val_accuracy: 0.6380\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5131 - accuracy: 0.9240 - val_loss: 0.9016 - val_accuracy: 0.6700\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.2761 - accuracy: 0.9527 - val_loss: 0.8317 - val_accuracy: 0.7200\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.1377 - accuracy: 0.9740 - val_loss: 0.7746 - val_accuracy: 0.7420\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0652 - accuracy: 0.9880 - val_loss: 0.8660 - val_accuracy: 0.7100\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0356 - accuracy: 0.9960 - val_loss: 0.8545 - val_accuracy: 0.7400\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0185 - accuracy: 0.9987 - val_loss: 0.8311 - val_accuracy: 0.7680\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.8119 - val_accuracy: 0.7840\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7979 - val_accuracy: 0.7840\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7701 - val_accuracy: 0.8140\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7659 - val_accuracy: 0.8160\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8258 - val_accuracy: 0.8040\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8035 - val_accuracy: 0.8120\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8286 - val_accuracy: 0.8020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1c3c0eaf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azx2OSqTQ76O",
        "colab_type": "text"
      },
      "source": [
        "### d) ... with stacked sequential layers (LSTM or GRU)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNzl6hH6P5Mi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "013df22b-27f8-4c09-9e58-bb05de1d3d66"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "model.add(GRU(256, return_sequences=True))\n",
        "model.add(GRU(256))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.6040 - accuracy: 0.2420 - val_loss: 1.5936 - val_accuracy: 0.3180\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.5442 - accuracy: 0.3800 - val_loss: 1.5095 - val_accuracy: 0.3660\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.2462 - accuracy: 0.5367 - val_loss: 1.2768 - val_accuracy: 0.4400\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.9151 - accuracy: 0.6053 - val_loss: 1.1652 - val_accuracy: 0.5220\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.6629 - accuracy: 0.7560 - val_loss: 1.0434 - val_accuracy: 0.5960\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3527 - accuracy: 0.9100 - val_loss: 1.0884 - val_accuracy: 0.6580\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1260 - accuracy: 0.9707 - val_loss: 1.2232 - val_accuracy: 0.7160\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0385 - accuracy: 0.9907 - val_loss: 1.5458 - val_accuracy: 0.7160\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0144 - accuracy: 0.9960 - val_loss: 1.6083 - val_accuracy: 0.7360\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 1.7385 - val_accuracy: 0.7240\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.8288 - val_accuracy: 0.7240\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 1.7428 - val_accuracy: 0.7240\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 1.8188 - val_accuracy: 0.7200\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0022 - accuracy: 0.9987 - val_loss: 1.9189 - val_accuracy: 0.7140\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 5.3695e-04 - accuracy: 1.0000 - val_loss: 1.8543 - val_accuracy: 0.7160\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 8.4811e-04 - accuracy: 1.0000 - val_loss: 1.8286 - val_accuracy: 0.7200\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.9776e-04 - accuracy: 1.0000 - val_loss: 2.0619 - val_accuracy: 0.7000\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 8.1835e-04 - accuracy: 0.9993 - val_loss: 1.7892 - val_accuracy: 0.7020\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 2.0834e-04 - accuracy: 1.0000 - val_loss: 1.7353 - val_accuracy: 0.7100\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.3045e-04 - accuracy: 1.0000 - val_loss: 1.7260 - val_accuracy: 0.7160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1c3b6e2a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e-yuKXKTIix",
        "colab_type": "text"
      },
      "source": [
        "### e) ...with bidirectional sequential layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjK9W0aLSzBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "c1b94843-8acb-4610-dc74-8a316f0cade9"
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "model.add(Bidirectional(GRU(256)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 1.6029 - accuracy: 0.2607 - val_loss: 1.5948 - val_accuracy: 0.2600\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.5463 - accuracy: 0.3967 - val_loss: 1.5531 - val_accuracy: 0.3440\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 1.3541 - accuracy: 0.4980 - val_loss: 1.2925 - val_accuracy: 0.4840\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.0185 - accuracy: 0.6073 - val_loss: 1.0819 - val_accuracy: 0.5720\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.6454 - accuracy: 0.8640 - val_loss: 0.9197 - val_accuracy: 0.6620\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2983 - accuracy: 0.9527 - val_loss: 0.8376 - val_accuracy: 0.6660\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1092 - accuracy: 0.9807 - val_loss: 0.8307 - val_accuracy: 0.7720\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0465 - accuracy: 0.9913 - val_loss: 1.1259 - val_accuracy: 0.6320\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0337 - accuracy: 0.9967 - val_loss: 0.9319 - val_accuracy: 0.7020\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0192 - accuracy: 0.9980 - val_loss: 0.9025 - val_accuracy: 0.6980\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.8825 - val_accuracy: 0.7460\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.0560 - val_accuracy: 0.7400\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0546 - val_accuracy: 0.7360\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 5.6447e-04 - accuracy: 1.0000 - val_loss: 0.9988 - val_accuracy: 0.7500\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 2.9313e-04 - accuracy: 1.0000 - val_loss: 0.9737 - val_accuracy: 0.7740\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.8332e-04 - accuracy: 1.0000 - val_loss: 0.9653 - val_accuracy: 0.7760\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.2022e-04 - accuracy: 1.0000 - val_loss: 0.9631 - val_accuracy: 0.7820\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 9.3783e-05 - accuracy: 1.0000 - val_loss: 0.9624 - val_accuracy: 0.7860\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 7.8862e-05 - accuracy: 1.0000 - val_loss: 0.9618 - val_accuracy: 0.7900\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 6.8648e-05 - accuracy: 1.0000 - val_loss: 0.9607 - val_accuracy: 0.7860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1c3a86feb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZkKLT6fHxUm",
        "colab_type": "text"
      },
      "source": [
        "### f) Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXnLlpcj6lps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d545df8b-fbfe-4297-d669-6c2582d297a3"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "model.add(GRU(256, dropout=0.5))\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/30\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 1.8050 - accuracy: 0.2260 - val_loss: 1.6179 - val_accuracy: 0.2340\n",
            "Epoch 2/30\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 1.6012 - accuracy: 0.2373 - val_loss: 1.6282 - val_accuracy: 0.2160\n",
            "Epoch 3/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 1.5966 - accuracy: 0.2387 - val_loss: 1.6082 - val_accuracy: 0.2160\n",
            "Epoch 4/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 1.5517 - accuracy: 0.2687 - val_loss: 1.5623 - val_accuracy: 0.3100\n",
            "Epoch 5/30\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 1.4417 - accuracy: 0.4767 - val_loss: 1.4119 - val_accuracy: 0.4380\n",
            "Epoch 6/30\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 1.1893 - accuracy: 0.5347 - val_loss: 1.3149 - val_accuracy: 0.4840\n",
            "Epoch 7/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 1.0101 - accuracy: 0.6647 - val_loss: 1.1595 - val_accuracy: 0.5220\n",
            "Epoch 8/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.7860 - accuracy: 0.7120 - val_loss: 1.0564 - val_accuracy: 0.6220\n",
            "Epoch 9/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.5665 - accuracy: 0.8887 - val_loss: 0.9436 - val_accuracy: 0.6740\n",
            "Epoch 10/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3614 - accuracy: 0.9213 - val_loss: 0.8553 - val_accuracy: 0.7040\n",
            "Epoch 11/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.2213 - accuracy: 0.9473 - val_loss: 0.8094 - val_accuracy: 0.7100\n",
            "Epoch 12/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.1402 - accuracy: 0.9667 - val_loss: 0.7858 - val_accuracy: 0.7460\n",
            "Epoch 13/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0965 - accuracy: 0.9767 - val_loss: 0.7350 - val_accuracy: 0.7740\n",
            "Epoch 14/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0510 - accuracy: 0.9940 - val_loss: 0.7747 - val_accuracy: 0.7500\n",
            "Epoch 15/30\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0363 - accuracy: 0.9953 - val_loss: 0.6766 - val_accuracy: 0.8040\n",
            "Epoch 16/30\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0251 - accuracy: 0.9947 - val_loss: 0.7977 - val_accuracy: 0.7680\n",
            "Epoch 17/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0222 - accuracy: 0.9980 - val_loss: 0.7804 - val_accuracy: 0.7660\n",
            "Epoch 18/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0135 - accuracy: 0.9987 - val_loss: 0.7288 - val_accuracy: 0.8100\n",
            "Epoch 19/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0088 - accuracy: 0.9993 - val_loss: 0.7464 - val_accuracy: 0.7740\n",
            "Epoch 20/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0070 - accuracy: 0.9993 - val_loss: 0.7397 - val_accuracy: 0.8060\n",
            "Epoch 21/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0055 - accuracy: 0.9993 - val_loss: 0.7476 - val_accuracy: 0.7960\n",
            "Epoch 22/30\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0084 - accuracy: 0.9993 - val_loss: 0.7331 - val_accuracy: 0.8140\n",
            "Epoch 23/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.7778 - val_accuracy: 0.8280\n",
            "Epoch 24/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0109 - accuracy: 0.9980 - val_loss: 0.7639 - val_accuracy: 0.7880\n",
            "Epoch 25/30\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.7104 - val_accuracy: 0.8140\n",
            "Epoch 26/30\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.7327 - val_accuracy: 0.8260\n",
            "Epoch 27/30\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7653 - val_accuracy: 0.8240\n",
            "Epoch 28/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7978 - val_accuracy: 0.8260\n",
            "Epoch 29/30\n",
            "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0087 - accuracy: 0.9967 - val_loss: 0.8454 - val_accuracy: 0.7880\n",
            "Epoch 30/30\n",
            "1500/1500 [==============================] - 2s 1ms/step - loss: 0.0074 - accuracy: 0.9993 - val_loss: 0.8136 - val_accuracy: 0.8120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1c3a5fdba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfNA3XkYMjeI",
        "colab_type": "text"
      },
      "source": [
        "## 4) Discuss 1) which model(s) performed best and speculate about 2) how you might try to further improve the predictive power of your model (e.g. Glove embeddings? More layers? Combining Conv1D with LSTM layers? More LSTM hidden nodes?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7myUvH9QMl9-",
        "colab_type": "text"
      },
      "source": [
        "1) Interestingly, the best performing model was a single unidirectional GRU layer of 256 neurons connected to a single dense layer of 256 neurons. One would think that a bidirectional model would be able to perform better since it processes the text in both directions, thus helping it fully understand the semantics of the text better, but nonetheless the simpler model did better. When dropout was added the model performed even better than its original version, albeit after training for 30 epochs instead of 20 (to make up for the fact that dropout increased the time to overfit).\n",
        "\n",
        "2) The model would certainly have improved with an imported, pre-trained embedding layer such as GloVe or Word2Vec, since these embeddings have already been trained on millions of texts, and so their word embeddings are more representative of the true semantic value of words than the freshly trained embeddings I used for the models in this analysis. I could have also combined 1D convolutional layers with GRU layers, since 1D convolutional layers can be used as both feature extractors and dimensionality reducers (when combined with MaxPooling, usually). Using a convolutional layer (or two) before the GRU layers would allow for a longer sequence of text to be processed, which would probably help the model understand more fully what exactly is being discussed in the text.\n",
        "\n",
        "I could also have increased the total number of layers of each model and/or increased the number of nodes per layer. However, the risk in those situations is overfitting, since larger models (more trainiable parameters) have a tendency to overfit more than smaller ones. Dropout could be used to mitigate some of this effect, but nonetheless overfitting is still an issue even with dropout. I did try models with larger parameter numbers, but the results were the same as the models above or worse, probably because of an overfitting tendency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkNwrvdyN9Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}